{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://i.ytimg.com/vi/dA_x2xHTYQE/maxresdefault.jpg)\n\n<font size='5' color='blue' align = 'center'>Table of Contents</font> \n<font size='3' color='purple'>\n1. [Introduction](#1)\n1. [**Manual Search**](#2)\n1. [**Grid Search**](#3)\n1. [**Random Search**](#4)\n1. [Automated Hyperparameter Tuning](#5)\n    1. [Bayesian Optimization using **HyperOpt**](#51)\n    1. [Genetic Algorithms using **TPOT**](#52)\n    1. [Artificial Neural Networks (ANNs) Tuning](#53)\n1. [**Optuna**](#6)    \n1. [**Tune**](#7)    \n1. [**Sherpa**](#8)    \n1. [Conclusion](#9)  "},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction <a id=\"1\"></a> <br>\n\n**Hyperparameter tuning** is choosing a set of optimal hyperparameters for a learning algorithm.\n\n**What is a hyperparameter?\n\n**A hyperparameter is a parameter whose value is set before the learning process begins.**\n\nSome examples of hyperparameters include penalty in logistic regression and loss in stochastic gradient descent.\n\nIn sklearn, hyperparameters are passed in as arguments to the constructor of the model classes.\n\nHyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n\nIt is possible and recommended to search the hyper-parameter space for the best Cross-validation i.e evaluating estimator performance score.\n\nAny parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, we can use the following method\n\nestimator.get_params()\n\nA search consists of:\n\n* an estimator (regressor or classifier such as sklearn.svm.SVC());\n* a parameter space;\n* a method for searching or sampling candidates;\n* a cross-validation scheme;\n* a score function.\n\nSome models allow for specialized, efficient parameter search strategies, outlined below.\n\nTwo generic approaches to sampling search candidates are provided in scikit-learn:\n![](https://developer.qualcomm.com/sites/default/files/attachments/learning_resources_03-05.png)\n**GridSearchCV** :For given values, GridSearchCV exhaustively considers all parameter combinations. The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter.\nFor instance, the following param_grid specifies that it has one grid to be explored that is a linear kernel with alpha values in [0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0009], and 'max_iter' i.e maximum 10000 iterations.\n\nparam_grid = {'alpha':[0.01,0.001,0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0009],'max_iter':[10000]}\n\n**RandomizedSearchCV**: It can sample a given number of candidates from a parameter space with a specified distribution.\nAfter describing these tools we detail best practice applicable to both approaches.\n\nNote that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommend to read the docstring of the estimator class to get a finer understanding of their expected behavior.\n\nI think it is enough of the theory .Now lets jump into practice.\n\nTo perform Hyperparameters Optimization in Python, we will use Credit Card Fraud Detection Dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\ndf = pd.read_csv('../input/creditcardfraud/creditcard.csv',na_values = '#NAME?')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[['V17', 'V9', 'V6', 'V12']]\nY = df['Class']","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30,random_state = 101)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Manual Search <a id=\"2\"></a> <br>\nWe will use a Random Forest Classifier as our model to optimize.Random Forest models are formed by a large number of uncorrelated decision trees, which joint together constitute an ensemble. In Random Forest, each decision tree makes its own prediction and the overall model output is selected to be the prediction which appeared most frequently.\n\nWe can now start by calculating our base model accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel = RandomForestClassifier(random_state= 101).fit(X_Train,Y_Train)\npredictionforest = model.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc1 = accuracy_score(Y_Test,predictionforest)","execution_count":4,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"[[85288    11]\n [   41   103]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85299\n           1       0.90      0.72      0.80       144\n\n    accuracy                           1.00     85443\n   macro avg       0.95      0.86      0.90     85443\nweighted avg       1.00      1.00      1.00     85443\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"When using Manual Search, we choose some model hyperparameters based on our judgment/experience. We then train the model, evaluate its accuracy and start the process again. This loop is repeated until a satisfactory accuracy is scored.\n\nThe main parameters used by a Random Forest Classifier are:\n\n* criterion = the function used to evaluate the quality of a split.\n* max_depth = maximum number of levels allowed in each tree.\n* max_features = maximum number of features considered when splitting a node.\n* min_samples_leaf = minimum number of samples which can be stored in a tree leaf.\n* min_samples_split = minimum number of samples necessary in a node to cause node splitting.\n* n_estimators = number of trees in the ensemble."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=10, random_state= 101).fit(X_Train,Y_Train)\npredictionforest = model.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc2 = accuracy_score(Y_Test,predictionforest)","execution_count":5,"outputs":[{"output_type":"stream","text":"[[85288    11]\n [   41   103]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85299\n           1       0.90      0.72      0.80       144\n\n    accuracy                           1.00     85443\n   macro avg       0.95      0.86      0.90     85443\nweighted avg       1.00      1.00      1.00     85443\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 3. Random Search <a id=\"3\"></a> <br>\n\nIn Random Search, we create a grid of hyperparameters and train/test our model on just some random combination of these hyperparameters. In this example, I additionally decided to perform Cross-Validation on the training set.\n\nWhen performing Machine Learning tasks, we generally divide our dataset in training and test sets. This is done so that to test our model after having trained it (in this way we can check it’s performances when working with unseen data). When using Cross-Validation, we divide our training set into N other partitions to make sure our model is not overfitting our data.\n\nOne of the most common used Cross-Validation methods is K-Fold Validation. In K-Fold, we divide our training set into N partitions and then iteratively train our model using N-1 partitions and test it with the left-over partition (at each iteration we change the left-over partition). Once having trained N times the model we then average the training results obtained in each iteration to obtain our overall training performance results.\n\nUsing Cross-Validation when implementing Hyperparameters optimization can be really important. In this way, we might avoid using some Hyperparameters which works really good on the training data but not so good with the test data.\nWe can now start implementing Random Search by first defying a grid of hyperparameters which will be randomly sampled when calling RandomizedSearchCV()."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nrandom_search = {'criterion': ['entropy', 'gini'],\n               'max_depth': [2],\n               'max_features': ['auto', 'sqrt'],\n               'min_samples_leaf': [4, 6, 8],\n               'min_samples_split': [5, 7,10],\n               'n_estimators': [20]}\n\nclf = RandomForestClassifier()\nmodel = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 10, \n                               cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\nmodel.fit(X_Train,Y_Train)","execution_count":6,"outputs":[{"output_type":"stream","text":"Fitting 4 folds for each of 10 candidates, totalling 40 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   42.0s finished\n","name":"stderr"},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"RandomizedSearchCV(cv=4, error_score='raise-deprecating',\n                   estimator=RandomForestClassifier(bootstrap=True,\n                                                    class_weight=None,\n                                                    criterion='gini',\n                                                    max_depth=None,\n                                                    max_features='auto',\n                                                    max_leaf_nodes=None,\n                                                    min_impurity_decrease=0.0,\n                                                    min_impurity_split=None,\n                                                    min_samples_leaf=1,\n                                                    min_samples_split=2,\n                                                    min_weight_fraction_leaf=0.0,\n                                                    n_estimators='warn',\n                                                    n_jobs=None,\n                                                    oob_sc...\n                                                    random_state=None,\n                                                    verbose=0,\n                                                    warm_start=False),\n                   iid='warn', n_iter=10, n_jobs=-1,\n                   param_distributions={'criterion': ['entropy', 'gini'],\n                                        'max_depth': [2],\n                                        'max_features': ['auto', 'sqrt'],\n                                        'min_samples_leaf': [4, 6, 8],\n                                        'min_samples_split': [5, 7, 10],\n                                        'n_estimators': [20]},\n                   pre_dispatch='2*n_jobs', random_state=101, refit=True,\n                   return_train_score=False, scoring=None, verbose=1)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Once trained our model, we can then visualize how changing some of its Hyperparameters can affect the overall model accuracy. In this case, I decided to observe how changing the number of estimators and the criterion can affect our Random Forest accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ntable = pd.pivot_table(pd.DataFrame(model.cv_results_),\n    values='mean_test_score', index='param_n_estimators', \n                       columns='param_criterion')\n     \nsns.heatmap(table)","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7fc92d873ef0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"We can now evaluate how our model performed using Random Search. In this case, using Random Search leads to a consistent increase in accuracy compared to our base model."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionforest = model.best_estimator_.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc3 = accuracy_score(Y_Test,predictionforest)","execution_count":8,"outputs":[{"output_type":"stream","text":"[[85279    20]\n [   46    98]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85299\n           1       0.83      0.68      0.75       144\n\n    accuracy                           1.00     85443\n   macro avg       0.91      0.84      0.87     85443\nweighted avg       1.00      1.00      1.00     85443\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 4. Grid Search <a id=\"4\"></a> <br>\nIn Grid Search, we set up a grid of hyperparameters and train/test our model on each of the possible combinations.\nIn order to choose the parameters to use in Grid Search, we can now look at which parameters worked best with Random Search and form a grid based on them to see if we can find a better combination.\n\nGrid Search can be implemented in Python using scikit-learn GridSearchCV() function."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid_search = {'criterion': ['entropy', 'gini'],\n               'max_depth': [2],\n               'max_features': ['auto', 'sqrt'],\n               'min_samples_leaf': [4, 6, 8],\n               'min_samples_split': [5, 7,10],\n               'n_estimators': [20]}\n\nclf = RandomForestClassifier()\nmodel = GridSearchCV(estimator = clf, param_grid = grid_search, \n                               cv = 4, verbose= 5, n_jobs = -1)\nmodel.fit(X_Train,Y_Train)\n\npredictionforest = model.best_estimator_.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc4 = accuracy_score(Y_Test,predictionforest)","execution_count":9,"outputs":[{"output_type":"stream","text":"Fitting 4 folds for each of 36 candidates, totalling 144 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   15.1s\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.4min\n[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:  2.6min finished\n","name":"stderr"},{"output_type":"stream","text":"[[85276    23]\n [   44   100]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85299\n           1       0.81      0.69      0.75       144\n\n    accuracy                           1.00     85443\n   macro avg       0.91      0.85      0.87     85443\nweighted avg       1.00      1.00      1.00     85443\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Grid Search is slower compared to Random Search but it can be overall more effective because it can go through the whole search space. Instead, Random Search can be faster fast but might miss some important points in the search space.\n# 5. Automated Hyperparameter Tuning <a id=\"5\"></a> <br>\n\n![](https://better.future-processing.com/directus/storage/uploads/2399317284eda5016daac68812d5d3c3.png)\nAs we have seen above tuning machine learning hyperparameters is indeed a tedious but crucial task, as the performance of an algorithm can be highly dependent on the choice of hyperparameters. Manual tuning takes time away from important steps of the machine learning pipeline like feature engineering and interpreting results. Grid and random search are hands-off, but require long run times because they waste time evaluating unpromising areas of the search space. Increasingly, hyperparameter tuning is done by automated methods that aim to find optimal hyperparameters in less time using an informed search with no manual effort necessary beyond the initial set-up.\n\nWhen using Automated Hyperparameter Tuning, the model hyperparameters to use are identified using techniques such as: Bayesian Optimization, Gradient Descent and Evolutionary Algorithms.\n\n## Bayesian Optimization using HyperOpt <a id=\"51\"></a> <br>\n\n![](https://i.imgur.com/BWbgCSx.jpg)\nBayesian optimization, a model-based method for finding the minimum of a function,while the final aim is to find the input value to a function which can give us the lowest possible output value has resulted in achieving better performance while requiring fewer iterations than random search.  Bayesian Optimization can, therefore, lead to better performance in the testing phase and reduced optimization time.\n\nBayesian Optimization can be performed in Python using the Hyperopt library.  \n\n![](https://camo.githubusercontent.com/b92ead141ef3726da38eef053864aa1173012789/68747470733a2f2f692e706f7374696d672e63632f54506d66665772702f68797065726f70742d6e65772e706e67)\n\nIn Hyperopt, Bayesian Optimization can be implemented giving 3 three main parameters to the function fmin().\n\n* **Objective Function** = defines the loss function to minimize.\n* **Domain Space** = defines the range of input values to test (in Bayesian Optimization this space creates a probability distribution for each of the used Hyperparameters).\n* **Optimization Algorithm** = defines the search algorithm to use to select the best input values to use in each new iteration.\n\nAdditionally, can also be defined in **fmin()** the maximum number of evaluations to perform.\n\nBayesian Optimization can reduce the number of search iterations by choosing the input values bearing in mind the past outcomes. In this way, we can concentrate our search from the beginning on values which are closer to our desired output.\nWe can now run our Bayesian Optimizer using the fmin() function. A Trials() object is first created to make possible to visualize later what was going on while the **fmin()** function was running (eg. how the loss function was changing and how to used Hyperparameters were changing).\n\n\nHyperopt is one of several automated hyperparameter tuning libraries using Bayesian optimization. These libraries differ in the algorithm used to both construct the surrogate (probability model) of the objective function and choose the next hyperparameters to evaluate in the objective function. Hyperopt uses the Tree Parzen Estimator (TPE). Other Python libraries include Spearmint, which uses a Gaussian process for the surrogate, and SMAC, which uses a random forest regression.\n\nHyperopt has a simple syntax for structuring an optimization problem which extends beyond hyperparameter tuning to any problem that involves minimizing a function."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install hyperopt","execution_count":10,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: hyperopt in /opt/conda/lib/python3.6/site-packages (0.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from hyperopt) (1.12.0)\nCollecting networkx==2.2 (from hyperopt)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/f4/7e20ef40b118478191cec0b58c3192f822cace858c19505c7670961b76b2/networkx-2.2.zip (1.7MB)\n\u001b[K     |████████████████████████████████| 1.7MB 2.8MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from hyperopt) (0.17.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from hyperopt) (4.32.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from hyperopt) (1.2.1)\nRequirement already satisfied: pymongo in /opt/conda/lib/python3.6/site-packages (from hyperopt) (3.9.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from hyperopt) (1.17.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx==2.2->hyperopt) (4.4.0)\nBuilding wheels for collected packages: networkx\n  Building wheel for networkx (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for networkx: filename=networkx-2.2-py2.py3-none-any.whl size=1527322 sha256=697d0b812883c80bbb1371d8f399a5d36337dad688bccda535a5bc06e504ccdc\n  Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91\nSuccessfully built networkx\n\u001b[31mERROR: osmnx 0.10 has requirement networkx>=2.3, but you'll have networkx 2.2 which is incompatible.\u001b[0m\nInstalling collected packages: networkx\n  Found existing installation: networkx 2.3\n    Uninstalling networkx-2.3:\n      Successfully uninstalled networkx-2.3\nSuccessfully installed networkx-2.2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n\nspace = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n        'max_depth': hp.quniform('max_depth', 10, 12, 10),\n        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n        'min_samples_leaf': hp.uniform ('min_samples_leaf', 0, 0.5),\n        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n        'n_estimators' : hp.choice('n_estimators', [10, 50])\n    }\n\ndef objective(space):\n    model = RandomForestClassifier(criterion = space['criterion'], \n                                   max_depth = space['max_depth'],\n                                 max_features = space['max_features'],\n                                 min_samples_leaf = space['min_samples_leaf'],\n                                 min_samples_split = space['min_samples_split'],\n                                 n_estimators = space['n_estimators'], \n                                 )\n    \n    accuracy = cross_val_score(model, X_Train, Y_Train, cv = 4).mean()\n\n    # We aim to maximize accuracy, therefore we return it as a negative value\n    return {'loss': -accuracy, 'status': STATUS_OK }\n    \ntrials = Trials()\nbest = fmin(fn= objective,\n            space= space,\n            algo= tpe.suggest,\n            max_evals = 20,\n            trials= trials)\nbest","execution_count":11,"outputs":[{"output_type":"stream","text":"100%|██████████| 20/20 [05:32<00:00, 28.33s/it, best loss: -0.9982544491482915]\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"{'criterion': 1,\n 'max_depth': 10.0,\n 'max_features': 1,\n 'min_samples_leaf': 0.3305594700288367,\n 'min_samples_split': 0.34955229007971167,\n 'n_estimators': 0}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We can now retrieve the set of **best** parameters identified and test our model using the **best** dictionary created during training. Some of the parameters have been stored in the **best** dictionary numerically using indices, therefore, we need first to convert them back as strings before input them in our Random Forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"crit = {0: 'entropy', 1: 'gini'}\nfeat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\nest = {0: 10, 1: 50, 2: 75, 3: 100, 4: 125}\n\ntrainedforest = RandomForestClassifier(criterion = crit[best['criterion']], \n                                       max_depth = best['max_depth'], \n                                       max_features = feat[best['max_features']], \n                                       min_samples_leaf = best['min_samples_leaf'], \n                                       min_samples_split = best['min_samples_split'], \n                                       n_estimators = est[best['n_estimators']]\n                                      ).fit(X_Train,Y_Train)\npredictionforest = trainedforest.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc5 = accuracy_score(Y_Test,predictionforest)","execution_count":12,"outputs":[{"output_type":"stream","text":"[[85299     0]\n [  144     0]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85299\n           1       0.00      0.00      0.00       144\n\n    accuracy                           1.00     85443\n   macro avg       0.50      0.50      0.50     85443\nweighted avg       1.00      1.00      1.00     85443\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Genetic Algorithms using TPOT <a id=\"52\"></a> <br>\nIn computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection.\n\nGenetic Algorithms tries to apply natural selection mechanisms to Machine Learning contexts. They are inspired by the Darwinian process of Natural Selection and they are therefore also usually called as Evolutionary Algorithms.\n![](https://mctrans.ce.ufl.edu/featured/TRANSYT-7F/release9/genetic2.gif)\nLet’s imagine we create a population of N Machine Learning models with some predefined Hyperparameters. We can then calculate the accuracy of each model and decide to keep just half of the models (the ones that perform best). We can now generate some **offsprings** having similar Hyperparameters to the ones of the best models so that to get again a population of N models. At this point, we can again calculate the accuracy of each model and repeat the cycle for a defined number of generations. In this way, just the best models will survive at the end of the process.\n\nIn order to implement Genetic Algorithms in Python, we can use the TPOT Auto Machine Learning library. TPOT is built on the scikit-learn library and it can be used for either regression or classification tasks."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pip install deap update_checker tqdm stopit","execution_count":13,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: deap in /opt/conda/lib/python3.6/site-packages (1.3.0)\nRequirement already satisfied: update_checker in /opt/conda/lib/python3.6/site-packages (0.16)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (4.32.1)\nRequirement already satisfied: stopit in /opt/conda/lib/python3.6/site-packages (1.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from deap) (1.17.0)\nRequirement already satisfied: requests>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from update_checker) (2.22.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.3.0->update_checker) (1.24.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.3.0->update_checker) (2019.6.16)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.3.0->update_checker) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.3.0->update_checker) (3.0.4)\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pip install tpot","execution_count":14,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: tpot in /opt/conda/lib/python3.6/site-packages (0.10.2)\nRequirement already satisfied: joblib>=0.10.3 in /opt/conda/lib/python3.6/site-packages (from tpot) (0.13.2)\nRequirement already satisfied: numpy>=1.12.1 in /opt/conda/lib/python3.6/site-packages (from tpot) (1.17.0)\nRequirement already satisfied: scikit-learn>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from tpot) (0.21.3)\nRequirement already satisfied: pandas>=0.20.2 in /opt/conda/lib/python3.6/site-packages (from tpot) (0.25.0)\nRequirement already satisfied: stopit>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from tpot) (1.1.2)\nRequirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from tpot) (1.2.1)\nRequirement already satisfied: deap>=1.0 in /opt/conda/lib/python3.6/site-packages (from tpot) (1.3.0)\nRequirement already satisfied: update-checker>=0.16 in /opt/conda/lib/python3.6/site-packages (from tpot) (0.16)\nRequirement already satisfied: tqdm>=4.26.0 in /opt/conda/lib/python3.6/site-packages (from tpot) (4.32.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.2->tpot) (2019.2)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.2->tpot) (2.8.0)\nRequirement already satisfied: requests>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from update-checker>=0.16->tpot) (2.22.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.20.2->tpot) (1.12.0)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2019.6.16)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.2)\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tpot import TPOTClassifier\n\nparameters = {'criterion': ['entropy', 'gini'],\n               'max_depth': [2],\n               'max_features': ['auto'],\n               'min_samples_leaf': [4, 12],\n               'min_samples_split': [5, 10],\n               'n_estimators': [10]}\n               \ntpot_classifier = TPOTClassifier(generations= 4, population_size= 24, offspring_size= 12,\n                                 verbosity= 2, early_stop= 12,\n                                 config_dict=\n                                 {'sklearn.ensemble.RandomForestClassifier': parameters}, \n                                 cv = 4, scoring = 'accuracy')\ntpot_classifier.fit(X_Train,Y_Train) ","execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Optimization Progress', max=72, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Generation 1 - Current best internal CV score: 0.9992476073915049\nGeneration 2 - Current best internal CV score: 0.9992476073915049\nGeneration 3 - Current best internal CV score: 0.9992476073915049\nGeneration 4 - Current best internal CV score: 0.9992476073915049\n\nBest pipeline: RandomForestClassifier(RandomForestClassifier(RandomForestClassifier(input_matrix, criterion=entropy, max_depth=2, max_features=auto, min_samples_leaf=12, min_samples_split=5, n_estimators=10), criterion=entropy, max_depth=2, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=10), criterion=entropy, max_depth=2, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=10)\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"TPOTClassifier(config_dict={'sklearn.ensemble.RandomForestClassifier': {'criterion': ['entropy',\n                                                                                      'gini'],\n                                                                        'max_depth': [2],\n                                                                        'max_features': ['auto'],\n                                                                        'min_samples_leaf': [4,\n                                                                                             12],\n                                                                        'min_samples_split': [5,\n                                                                                              10],\n                                                                        'n_estimators': [10]}},\n               crossover_rate=0.1, cv=4, disable_update_check=False,\n               early_stop=12, generations=4, max_eval_time_mins=5,\n               max_time_mins=None, memory=None, mutation_rate=0.9, n_jobs=1,\n               offspring_size=12, periodic_checkpoint_folder=None,\n               population_size=24, random_state=None, scoring='accuracy',\n               subsample=1.0, template=None, use_dask=False, verbosity=2,\n               warm_start=False)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The training report and the best parameters are identified above using Genetic Algorithms. \n\nThe overall accuracy of our Random Forest Genetic Algorithm optimized model is shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = tpot_classifier.score(X_Test, Y_Test)\nprint(accuracy)","execution_count":16,"outputs":[{"output_type":"stream","text":"0.999204147794436\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Artificial Neural Networks (ANNs) Tuning <a id=\"53\"></a> <br>\n![](https://miro.medium.com/max/6000/1*wT6pIMnjZ9oArkidnVsGtg.png)\nUsing KerasClassifier wrapper, it is possible to apply Grid Search and Random Search for Deep Learning models in the same way it was done when using scikit-learn Machine Learning models. In the following example, we will try to optimize some of our ANN parameters such as: how many neurons to use in each layer and which activation function and optimizer to use."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier","execution_count":17,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def DL_Model(activation= 'linear', neurons= 5, optimizer='Adam'):\n    model = Sequential()\n    model.add(Dense(neurons, input_dim= 4, activation= activation))\n    model.add(Dense(neurons, activation= activation))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n    return model","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining grid parameters\nactivation = ['softmax', 'relu']\nneurons = [5, 10]\noptimizer = ['Adam', 'Adamax']\nparam_grid = dict(activation = activation, neurons = neurons, optimizer = optimizer)\n\nclf = KerasClassifier(build_fn= DL_Model, epochs= 5, batch_size=1024, verbose= 2)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GridSearchCV(estimator= clf, param_grid=param_grid, n_jobs=-1)\nmodel.fit(X_Train,Y_Train)","execution_count":20,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n  warnings.warn(CV_WARNING, FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"Epoch 1/5\n - 1s - loss: 0.4816 - acc: 0.9094\nEpoch 2/5\n - 1s - loss: 0.2698 - acc: 0.9982\nEpoch 3/5\n - 1s - loss: 0.1383 - acc: 0.9983\nEpoch 4/5\n - 1s - loss: 0.0657 - acc: 0.9983\nEpoch 5/5\n - 1s - loss: 0.0438 - acc: 0.9983\n","name":"stdout"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"GridSearchCV(cv='warn', error_score='raise-deprecating',\n             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fc8f5c12a20>,\n             iid='warn', n_jobs=-1,\n             param_grid={'activation': ['softmax', 'relu'], 'neurons': [5, 10],\n                         'optimizer': ['Adam', 'Adamax']},\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring=None, verbose=0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Max Accuracy Registred: {} using {}\".format(round(model.best_score_,3), \n                                                   model.best_params_))","execution_count":21,"outputs":[{"output_type":"stream","text":"Max Accuracy Registred: 0.999 using {'activation': 'relu', 'neurons': 5, 'optimizer': 'Adam'}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The overall accuracy scored using our Artificial Neural Network (ANN) can be viewed below."},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_test = model.predict(X_Test)\nprint(confusion_matrix(Y_Test,prediction_test))\nprint(classification_report(Y_Test,prediction_test))\naccuracy_ANN = accuracy_score(Y_Test,prediction_test)","execution_count":22,"outputs":[{"output_type":"stream","text":"[[85299     0]\n [  144     0]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85299\n           1       0.00      0.00      0.00       144\n\n    accuracy                           1.00     85443\n   macro avg       0.50      0.50      0.50     85443\nweighted avg       1.00      1.00      1.00     85443\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# 6.Optuna <a id=\"6\"></a> <br>\n![](https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png)\n\nOptuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. Optuna is a framework designed for the automation and the acceleration of the optimization studies.\n\n**Key Features:**\n\n* **Eager search spaces**: Automated search for optimal hyperparameters using Python conditionals, loops, and syntax\n\n* **State-of-the-art algorithms**: Efficiently search large spaces and prune unpromising trials for faster results\n\n* **Easy parallelization**: Parallelize hyperparameter searches over multiple threads or processes without modifying code\n\nWe use the terms **study** and **trial** as follows:\n\n**Study**: optimization based on an objective function\n\n**Trial**: a single execution of the objective function\n\nThe goal of a study is to find out the optimal set of hyperparameter values (e.g., classifier and svm_c) through multiple trials (e.g., n_trials=100). \n\n"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install optuna","execution_count":23,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: optuna in /opt/conda/lib/python3.6/site-packages (0.14.0)\nRequirement already satisfied: cliff in /opt/conda/lib/python3.6/site-packages (from optuna) (2.15.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from optuna) (1.17.0)\nRequirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from optuna) (1.3.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from optuna) (1.12.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from optuna) (1.2.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from optuna) (0.25.0)\nRequirement already satisfied: alembic in /opt/conda/lib/python3.6/site-packages (from optuna) (1.0.11)\nRequirement already satisfied: typing in /opt/conda/lib/python3.6/site-packages (from optuna) (3.6.6)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.6/site-packages (from optuna) (4.0.2)\nRequirement already satisfied: pyparsing>=2.1.0 in /opt/conda/lib/python3.6/site-packages (from cliff->optuna) (2.4.2)\nRequirement already satisfied: PrettyTable<0.8,>=0.7.2 in /opt/conda/lib/python3.6/site-packages (from cliff->optuna) (0.7.2)\nRequirement already satisfied: cmd2!=0.8.3; python_version >= \"3.0\" in /opt/conda/lib/python3.6/site-packages (from cliff->optuna) (0.9.16)\nRequirement already satisfied: PyYAML>=3.12 in /opt/conda/lib/python3.6/site-packages (from cliff->optuna) (5.1.2)\nRequirement already satisfied: stevedore>=1.20.0 in /opt/conda/lib/python3.6/site-packages (from cliff->optuna) (1.30.1)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from cliff->optuna) (5.4.2)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->optuna) (2.8.0)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->optuna) (2019.2)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.6/site-packages (from alembic->optuna) (1.1.0)\nRequirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.6/site-packages (from alembic->optuna) (1.0.4)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.6/site-packages (from cmd2!=0.8.3; python_version >= \"3.0\"->cliff->optuna) (1.7.0)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.6/site-packages (from cmd2!=0.8.3; python_version >= \"3.0\"->cliff->optuna) (0.1.7)\nRequirement already satisfied: colorama>=0.3.7 in /opt/conda/lib/python3.6/site-packages (from cmd2!=0.8.3; python_version >= \"3.0\"->cliff->optuna) (0.4.1)\nRequirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.6/site-packages (from cmd2!=0.8.3; python_version >= \"3.0\"->cliff->optuna) (19.1.0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.6/site-packages (from Mako->alembic->optuna) (1.1.1)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We can optimize Scikit-Learn hyperparameters, such as the C parameter of SVC and the max_depth of the RandomForestClassifier, in three steps:\n\n* Wrap model training with an objective function and return accuracy\n* Suggest hyperparameters using a trial object\n* Create a study object and execute the optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nimport sklearn.datasets\nimport sklearn.ensemble\nimport sklearn.model_selection\nimport sklearn.svm\nimport optuna\n\n# 1. Define an objective function to be maximized.\ndef objective(trial):\n    iris = sklearn.datasets.load_iris()\n    x, y = iris.data, iris.target\n    # 2. Suggest values for the hyperparameters using a trial object.\n    classifier_name = trial.suggest_categorical('classifier', ['SVC', 'RandomForest'])\n    if classifier_name == 'SVC':\n         svc_c = trial.suggest_loguniform('svc_c', 1e-10, 1e10)\n         classifier_obj = sklearn.svm.SVC(C=svc_c, gamma='auto')\n    else:\n        rf_max_depth = int(trial.suggest_loguniform('rf_max_depth', 2, 32))\n        classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=rf_max_depth, n_estimators=10)\n    ...\n    return accuracy\n\n\n# 3. Create a study object and optimize the objective function.\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)","execution_count":24,"outputs":[{"output_type":"stream","text":"[I 2020-08-17 09:38:33,131] Finished trial#0 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:33,315] Finished trial#1 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:33,499] Finished trial#2 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:33,683] Finished trial#3 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:33,866] Finished trial#4 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:34,052] Finished trial#5 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:34,237] Finished trial#6 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:34,423] Finished trial#7 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:34,613] Finished trial#8 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:34,796] Finished trial#9 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:35,015] Finished trial#10 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:35,202] Finished trial#11 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:35,388] Finished trial#12 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:35,572] Finished trial#13 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:35,763] Finished trial#14 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:35,953] Finished trial#15 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:36,147] Finished trial#16 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:36,345] Finished trial#17 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:36,540] Finished trial#18 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:36,735] Finished trial#19 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:36,929] Finished trial#20 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:37,127] Finished trial#21 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:37,323] Finished trial#22 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:37,519] Finished trial#23 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:37,714] Finished trial#24 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:37,910] Finished trial#25 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:38,108] Finished trial#26 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:38,301] Finished trial#27 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:38,494] Finished trial#28 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:38,689] Finished trial#29 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:38,883] Finished trial#30 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:39,080] Finished trial#31 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:39,277] Finished trial#32 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:39,475] Finished trial#33 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:39,672] Finished trial#34 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:39,876] Finished trial#35 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:40,078] Finished trial#36 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:40,276] Finished trial#37 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:40,474] Finished trial#38 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:40,674] Finished trial#39 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:40,870] Finished trial#40 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n","name":"stderr"},{"output_type":"stream","text":"[I 2020-08-17 09:38:41,071] Finished trial#41 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:41,272] Finished trial#42 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:41,473] Finished trial#43 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:41,674] Finished trial#44 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:41,880] Finished trial#45 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:42,087] Finished trial#46 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:42,291] Finished trial#47 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:42,495] Finished trial#48 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:42,697] Finished trial#49 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:42,904] Finished trial#50 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:43,111] Finished trial#51 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:43,323] Finished trial#52 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:43,527] Finished trial#53 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:43,736] Finished trial#54 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:43,942] Finished trial#55 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:44,152] Finished trial#56 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:44,361] Finished trial#57 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:44,575] Finished trial#58 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:44,785] Finished trial#59 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:44,995] Finished trial#60 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:45,204] Finished trial#61 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:45,417] Finished trial#62 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:45,631] Finished trial#63 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:45,851] Finished trial#64 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:46,095] Finished trial#65 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:46,314] Finished trial#66 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:46,531] Finished trial#67 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:46,754] Finished trial#68 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:46,976] Finished trial#69 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:47,194] Finished trial#70 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:47,409] Finished trial#71 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:47,624] Finished trial#72 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:47,843] Finished trial#73 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:48,061] Finished trial#74 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:48,280] Finished trial#75 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:48,500] Finished trial#76 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:48,718] Finished trial#77 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:48,936] Finished trial#78 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:49,160] Finished trial#79 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:49,381] Finished trial#80 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:49,600] Finished trial#81 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n","name":"stderr"},{"output_type":"stream","text":"[I 2020-08-17 09:38:49,821] Finished trial#82 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:50,044] Finished trial#83 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:50,267] Finished trial#84 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:50,489] Finished trial#85 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:50,713] Finished trial#86 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:50,933] Finished trial#87 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:51,155] Finished trial#88 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:51,379] Finished trial#89 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:51,600] Finished trial#90 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:51,821] Finished trial#91 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:52,043] Finished trial#92 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:52,267] Finished trial#93 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:52,490] Finished trial#94 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:52,715] Finished trial#95 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:52,945] Finished trial#96 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:53,179] Finished trial#97 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:53,413] Finished trial#98 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n[I 2020-08-17 09:38:53,644] Finished trial#99 resulted in value: 0.999204147794436. Current best value is 0.999204147794436 with parameters: {'classifier': 'SVC', 'svc_c': 2.054891444107311e-09}.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# 7. Tune <a id=\"7\"></a> <br>\n![](https://miro.medium.com/max/3622/1*GsJLYcS5W2tCfHg4NDOscA.png)\nTune is a Python library for experiment execution and hyperparameter tuning at any scale. Core features:\n\n* Launch a multi-node distributed hyperparameter sweep in less than 10 lines of code.\n\n* Supports any machine learning framework, including PyTorch, XGBoost, MXNet, and Keras.\n\n* Natively integrates with optimization libraries such as HyperOpt, Bayesian Optimization, and Facebook Ax.\n\n* Choose among scalable algorithms such as Population Based Training (PBT), Vizier’s Median Stopping Rule, HyperBand/ASHA.\n\n* Visualize results with TensorBoard.\n\n* Move your models from training to serving on the same infrastructure with Ray Serve."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install 'ray[tune]' torch torchvision","execution_count":25,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: ray[tune] in /opt/conda/lib/python3.6/site-packages (0.7.3)\n  WARNING: ray 0.7.3 does not provide the extra 'tune'\nRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (1.1.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (0.3.0)\nRequirement already satisfied: pytest in /opt/conda/lib/python3.6/site-packages (from ray[tune]) (5.0.1)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.6/site-packages (from ray[tune]) (0.4.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from ray[tune]) (3.0.12)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from ray[tune]) (5.1.2)\nRequirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.6/site-packages (from ray[tune]) (1.17.0)\nRequirement already satisfied: redis in /opt/conda/lib/python3.6/site-packages (from ray[tune]) (3.3.8)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from ray[tune]) (7.0)\nCollecting protobuf>=3.8.0 (from ray[tune])\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/79/510974552cebff2ba04038544799450defe75e96ea5f1675dbf72cc8744f/protobuf-3.13.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n     |████████████████████████████████| 1.3MB 2.7MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from ray[tune]) (1.12.0)\nRequirement already satisfied: funcsigs in /opt/conda/lib/python3.6/site-packages (from ray[tune]) (1.0.2)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision) (5.4.1)\nRequirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest->ray[tune]) (1.8.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from pytest->ray[tune]) (19.1)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest->ray[tune]) (19.1.0)\nRequirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from pytest->ray[tune]) (7.2.0)\nRequirement already satisfied: atomicwrites>=1.0 in /opt/conda/lib/python3.6/site-packages (from pytest->ray[tune]) (1.3.0)\nRequirement already satisfied: pluggy<1.0,>=0.12 in /opt/conda/lib/python3.6/site-packages (from pytest->ray[tune]) (0.12.0)\nRequirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.6/site-packages (from pytest->ray[tune]) (0.19)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from pytest->ray[tune]) (0.1.7)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->ray[tune]) (41.0.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->pytest->ray[tune]) (2.4.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=0.12->pytest->ray[tune]) (0.5.2)\nERROR: allennlp 0.8.4 requires awscli>=1.11.91, which is not installed.\nERROR: allennlp 0.8.4 requires flaky, which is not installed.\nERROR: allennlp 0.8.4 requires responses>=0.7, which is not installed.\nERROR: ethnicolr 0.2.1 has requirement tensorflow==1.12.3, but you'll have tensorflow 1.14.0 which is incompatible.\nERROR: chainer 6.2.0 has requirement protobuf<3.8.0rc1,>=3.0.0, but you'll have protobuf 3.13.0 which is incompatible.\nInstalling collected packages: protobuf\n  Found existing installation: protobuf 3.7.1\n    Uninstalling protobuf-3.7.1:\n      Successfully uninstalled protobuf-3.7.1\nSuccessfully installed protobuf-3.13.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\nfrom ray import tune\nfrom ray.tune.examples.mnist_pytorch import get_data_loaders, train, test","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_mnist(config):\n    train_loader, test_loader = get_data_loaders()\n    model = ConvNet()\n    optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n    for i in range(10):\n        train(model, optimizer, train_loader)\n        acc = test(model, test_loader)\n        tune.report(mean_accuracy=acc)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis = tune.run(train_mnist, config={\"lr\": tune.grid_search([0.001, 0.01, 0.1])})\nprint(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))","execution_count":28,"outputs":[{"output_type":"stream","text":"2020-08-17 09:39:19,786\tWARNING worker.py:1373 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n2020-08-17 09:39:19,790\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-08-17_09-39-19_788971_17/logs.\n2020-08-17 09:39:19,916\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:24889 to respond...\n2020-08-17 09:39:20,069\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:46884 to respond...\n2020-08-17 09:39:20,075\tINFO services.py:809 -- Starting Redis shard with 3.44 GB max memory.\n2020-08-17 09:39:20,148\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-08-17_09-39-19_788971_17/logs.\n2020-08-17 09:39:20,154\tINFO services.py:1475 -- Starting the Plasma object store with 5.15 GB memory using /dev/shm.\n2020-08-17 09:39:20,316\tINFO function_runner.py:249 -- tune.track signature detected.\n2020-08-17 09:39:20,366\tINFO trial_runner.py:176 -- Starting a new experiment.\n","name":"stderr"},{"output_type":"stream","text":"== Status ==\nUsing FIFO scheduling algorithm.\nResources requested: 0/4 CPUs, 0/0 GPUs\nMemory usage on this node: 3.0/20.0 GB\n\n","name":"stdout"},{"output_type":"stream","text":"2020-08-17 09:39:20,474\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n2020-08-17 09:39:20,577\tWARNING util.py:145 -- The `start_trial` operation took 0.14470314979553223 seconds to complete, which may be a performance bottleneck.\n","name":"stderr"},{"output_type":"stream","text":"== Status ==\nUsing FIFO scheduling algorithm.\nResources requested: 1/4 CPUs, 0/0 GPUs\nMemory usage on this node: 3.0/20.0 GB\nResult logdir: /root/ray_results/train_mnist\nNumber of trials: 3 ({'RUNNING': 1, 'PENDING': 2})\nPENDING trials:\n - train_mnist_1_lr=0.01:\tPENDING\n - train_mnist_2_lr=0.1:\tPENDING\nRUNNING trials:\n - train_mnist_0_lr=0.001:\tRUNNING\n\n","name":"stdout"},{"output_type":"stream","text":"2020-08-17 09:39:20,696\tWARNING util.py:145 -- The `start_trial` operation took 0.10655379295349121 seconds to complete, which may be a performance bottleneck.\n2020-08-17 09:39:20,821\tWARNING util.py:145 -- The `start_trial` operation took 0.10180950164794922 seconds to complete, which may be a performance bottleneck.\n","name":"stderr"},{"output_type":"stream","text":"(pid=303) \n(pid=303) 0it [00:00, ?it/s]\n(pid=303) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/data/MNIST/raw/train-images-idx3-ubyte.gz\n(pid=303) \n(pid=303)   0%|          | 0/9912422 [00:00<?, ?it/s]\n(pid=303) \n(pid=303)   2%|▏         | 212992/9912422 [00:00<00:04, 2062047.47it/s]\n(pid=303) \n(pid=303)  70%|██████▉   | 6897664/9912422 [00:00<00:01, 2907335.16it/s]\n(pid=303) \n(pid=303) 9920512it [00:00, 28455532.58it/s]                            \n(pid=303) Extracting /root/data/MNIST/raw/train-images-idx3-ubyte.gz\n(pid=301) Using downloaded and verified file: /root/data/MNIST/raw/train-images-idx3-ubyte.gz\n(pid=301) Extracting /root/data/MNIST/raw/train-images-idx3-ubyte.gz\n(pid=302) Using downloaded and verified file: /root/data/MNIST/raw/train-images-idx3-ubyte.gz\n(pid=302) Extracting /root/data/MNIST/raw/train-images-idx3-ubyte.gz\n(pid=303) \n(pid=303) 0it [00:00, ?it/s]\n(pid=302) 2020-08-17 09:39:25,778\tERROR function_runner.py:98 -- Runner Thread raised error.\n(pid=302) Traceback (most recent call last):\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 92, in run\n(pid=302)     self._entrypoint()\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 140, in entrypoint\n(pid=302)     return self._trainable_func(config, self._status_reporter)\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 266, in _trainable_func\n(pid=302)     output = train_func(config)\n(pid=302)   File \"<ipython-input-27-7ca1dfcabe9c>\", line 2, in train_mnist\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/examples/mnist_pytorch.py\", line 75, in get_data_loaders\n(pid=302)     \"~/data\", train=True, download=True, transform=mnist_transforms),\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 68, in __init__\n(pid=302)     self.download()\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 146, in download\n(pid=302)     self.extract_gzip(gzip_path=file_path, remove_finished=True)\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 130, in extract_gzip\n(pid=302)     os.unlink(gzip_path)\n(pid=302) FileNotFoundError: [Errno 2] No such file or directory: '/root/data/MNIST/raw/train-images-idx3-ubyte.gz'\n(pid=302) Exception in thread Thread-1:\n(pid=302) Traceback (most recent call last):\n(pid=302)   File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n(pid=302)     self.run()\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 111, in run\n(pid=302)     raise e\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 92, in run\n(pid=302)     self._entrypoint()\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 140, in entrypoint\n(pid=302)     return self._trainable_func(config, self._status_reporter)\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 266, in _trainable_func\n(pid=302)     output = train_func(config)\n(pid=302)   File \"<ipython-input-27-7ca1dfcabe9c>\", line 2, in train_mnist\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/examples/mnist_pytorch.py\", line 75, in get_data_loaders\n(pid=302)     \"~/data\", train=True, download=True, transform=mnist_transforms),\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 68, in __init__\n(pid=302)     self.download()\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 146, in download\n(pid=302)     self.extract_gzip(gzip_path=file_path, remove_finished=True)\n(pid=302)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 130, in extract_gzip\n(pid=302)     os.unlink(gzip_path)\n(pid=302) FileNotFoundError: [Errno 2] No such file or directory: '/root/data/MNIST/raw/train-images-idx3-ubyte.gz'\n(pid=302) \n(pid=303) Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/data/MNIST/raw/train-labels-idx1-ubyte.gz\n(pid=301) 2020-08-17 09:39:25,797\tERROR function_runner.py:98 -- Runner Thread raised error.\n(pid=301) Traceback (most recent call last):\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 92, in run\n(pid=301)     self._entrypoint()\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 140, in entrypoint\n(pid=301)     return self._trainable_func(config, self._status_reporter)\n","name":"stdout"},{"output_type":"stream","text":"2020-08-17 09:39:25,920\tERROR trial_runner.py:550 -- Error processing event.\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 498, in _process_trial\n    result = self.trial_executor.fetch_result(trial)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 342, in fetch_result\n    result = ray.get(trial_future[0])\n  File \"/opt/conda/lib/python3.6/site-packages/ray/worker.py\", line 2247, in get\n    raise value\nray.exceptions.RayTaskError: ","name":"stderr"},{"output_type":"stream","text":"(pid=301)","name":"stdout"},{"output_type":"stream","text":"ray_worker","name":"stderr"},{"output_type":"stream","text":"   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 266, in _trainable_func","name":"stdout"},{"output_type":"stream","text":" (pid=301, host=59a132d236e4)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/trainable.py\", line 171, in train\n    result = self._train()\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 194, in _train\n    self._report_thread_runner_error(block=True)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 237, in _report_thread_runner_error\n    .format(err_tb_str)))\nray.tune.error.TuneError: Trial raised an exception. Traceback:\n","name":"stderr"},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"ray_worker","name":"stderr"},{"output_type":"stream","text":"(pid=301)     output = train_func(config)","name":"stdout"},{"output_type":"stream","text":" (pid=301, host=59a132d236e4)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 92, in run\n    self._entrypoint()\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 140, in entrypoint\n    return self._trainable_func(config, self._status_reporter)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 266, in _trainable_func\n    output = train_func(config)\n  File \"<ipython-input-27-7ca1dfcabe9c>\", line 2, in train_mnist\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/examples/mnist_pytorch.py\", line 75, in get_data_loaders\n    \"~/data\", train=True, download=True, transform=mnist_transforms),\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 68, in __init__\n    self.download()\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 146, in download\n    self.extract_gzip(gzip_path=file_path, remove_finished=True)\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 130, in extract_gzip\n    os.unlink(gzip_path)\nFileNotFoundError: [Errno 2] No such file or directory: '/root/data/MNIST/raw/train-images-idx3-ubyte.gz'\n\n\n","name":"stderr"},{"output_type":"stream","text":"\n(pid=301)   File \"<ipython-input-27-7ca1dfcabe9c>\", line 2, in train_mnist\n== Status ==\nUsing FIFO scheduling algorithm.\nResources requested: 2/4 CPUs, 0/0 GPUs\nMemory usage on this node: 3.4/20.0 GB\nResult logdir: /root/ray_results/train_mnist\nNumber of trials: 3 ({'RUNNING': 2, 'ERROR': 1})\nERROR trials:\n - train_mnist_1_lr=0.01:\tERROR, 1 failures: /root/ray_results/train_mnist/train_mnist_1_lr=0.01_2020-08-17_09-39-20ctumttjq/error_2020-08-17_09-39-25.txt\nRUNNING trials:\n - train_mnist_0_lr=0.001:\tRUNNING\n - train_mnist_2_lr=0.1:\tRUNNING\n(pid=301)\n   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/examples/mnist_pytorch.py\", line 75, in get_data_loaders\n(pid=301)     \"~/data\", train=True, download=True, transform=mnist_transforms),\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 68, in __init__\n(pid=301)     self.download()","name":"stdout"},{"output_type":"stream","text":"2020-08-17 09:39:25,963\tERROR trial_runner.py:550 -- Error processing event.\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 498, in _process_trial\n    result = self.trial_executor.fetch_result(trial)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 342, in fetch_result\n    result = ray.get(trial_future[0])\n  File \"/opt/conda/lib/python3.6/site-packages/ray/worker.py\", line 2247, in get\n    raise value\nray.exceptions.RayTaskError: ray_worker","name":"stderr"},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":" (pid=302, host=59a132d236e4)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/trainable.py\", line 171, in train\n    result = self._train()\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 194, in _train\n    self._report_thread_runner_error(block=True)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 237, in _report_thread_runner_error\n    .format(err_tb_str)))\nray.tune.error.TuneError: Trial raised an exception. Traceback:\n","name":"stderr"},{"output_type":"stream","text":"(pid=301)","name":"stdout"},{"output_type":"stream","text":"ray_worker","name":"stderr"},{"output_type":"stream","text":"   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 146, in download","name":"stdout"},{"output_type":"stream","text":" (pid=302, host=59a132d236e4)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 92, in run\n    self._entrypoint()\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 140, in entrypoint\n    return self._trainable_func(config, self._status_reporter)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 266, in _trainable_func\n    output = train_func(config)\n  File \"<ipython-input-27-7ca1dfcabe9c>\", line 2, in train_mnist\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/examples/mnist_pytorch.py\", line 75, in get_data_loaders\n    \"~/data\", train=True, download=True, transform=mnist_transforms),\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 68, in __init__\n    self.download()\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 146, in download\n    self.extract_gzip(gzip_path=file_path, remove_finished=True)\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 130, in extract_gzip\n    os.unlink(gzip_path)\nFileNotFoundError: [Errno 2] No such file or directory: '/root/data/MNIST/raw/train-images-idx3-ubyte.gz'\n\n","name":"stderr"},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"},{"output_type":"stream","text":"(pid=301)     self.extract_gzip(gzip_path=file_path, remove_finished=True)\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 130, in extract_gzip\n(pid=301)     os.unlink(gzip_path)\n(pid=301) FileNotFoundError: [Errno 2] No such file or directory: '/root/data/MNIST/raw/train-images-idx3-ubyte.gz'\n(pid=301) Exception in thread Thread-1:\n(pid=301) Traceback (most recent call last):\n(pid=301)   File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n(pid=301)     self.run()\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 111, in run\n(pid=301)     raise e\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 92, in run\n(pid=301)     self._entrypoint()\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 140, in entrypoint\n(pid=301)     return self._trainable_func(config, self._status_reporter)\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 266, in _trainable_func\n(pid=301)     output = train_func(config)\n(pid=301)   File \"<ipython-input-27-7ca1dfcabe9c>\", line 2, in train_mnist\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/examples/mnist_pytorch.py\", line 75, in get_data_loaders\n(pid=301)     \"~/data\", train=True, download=True, transform=mnist_transforms),\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 68, in __init__\n(pid=301)     self.download()\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 146, in download\n(pid=301)     self.extract_gzip(gzip_path=file_path, remove_finished=True)\n(pid=301)   File \"/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py\", line 130, in extract_gzip\n(pid=301)     os.unlink(gzip_path)\n(pid=301) FileNotFoundError: [Errno 2] No such file or directory: '/root/data/MNIST/raw/train-images-idx3-ubyte.gz'\n(pid=301) \n(pid=303) \n(pid=303)  57%|█████▋    | 16384/28881 [00:00<00:00, 157272.24it/s]\n(pid=303) 32768it [00:00, 312683.18it/s]                           \n(pid=303) \n(pid=303) 0it [00:00, ?it/s]\n(pid=303) Extracting /root/data/MNIST/raw/train-labels-idx1-ubyte.gz\n(pid=303) Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n(pid=303) \n(pid=303)   1%|          | 16384/1648877 [00:00<00:14, 114552.13it/s]\n(pid=303) \n(pid=303)  92%|█████████▏| 1515520/1648877 [00:00<00:00, 163109.59it/s]\n(pid=303) 1654784it [00:00, 6741208.28it/s]                            \n(pid=303) Extracting /root/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n(pid=303) \n(pid=303) 0it [00:00, ?it/s]\n(pid=303) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n(pid=303) \n(pid=303) 8192it [00:00, 108240.79it/s]\n(pid=303) Extracting /root/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n(pid=303) Processing...\n(pid=303) Done!\n(pid=303) 2020-08-17 09:39:26,500\tERROR function_runner.py:98 -- Runner Thread raised error.\n(pid=303) Traceback (most recent call last):\n(pid=303)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 92, in run\n(pid=303)     self._entrypoint()\n(pid=303)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 140, in entrypoint\n(pid=303)     return self._trainable_func(config, self._status_reporter)\n(pid=303)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 266, in _trainable_func\n(pid=303)     output = train_func(config)\n(pid=303)   File \"<ipython-input-27-7ca1dfcabe9c>\", line 3, in train_mnist\n(pid=303) NameError: name 'ConvNet' is not defined\n(pid=303) Exception in thread Thread-1:\n(pid=303) Traceback (most recent call last):\n(pid=303)   File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n(pid=303)     self.run()\n(pid=303)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 111, in run\n(pid=303)     raise e\n(pid=303)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 92, in run\n(pid=303)     self._entrypoint()\n(pid=303)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 140, in entrypoint\n(pid=303)     return self._trainable_func(config, self._status_reporter)\n(pid=303)   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 266, in _trainable_func\n(pid=303)     output = train_func(config)\n(pid=303)   File \"<ipython-input-27-7ca1dfcabe9c>\", line 3, in train_mnist\n(pid=303) NameError: name 'ConvNet' is not defined\n(pid=303) \n","name":"stdout"},{"output_type":"stream","text":"2020-08-17 09:39:26,656\tERROR trial_runner.py:550 -- Error processing event.\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 498, in _process_trial\n    result = self.trial_executor.fetch_result(trial)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 342, in fetch_result\n    result = ray.get(trial_future[0])\n  File \"/opt/conda/lib/python3.6/site-packages/ray/worker.py\", line 2247, in get\n    raise value\nray.exceptions.RayTaskError: ray_worker (pid=303, host=59a132d236e4)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/trainable.py\", line 171, in train\n    result = self._train()\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 194, in _train\n    self._report_thread_runner_error(block=True)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 237, in _report_thread_runner_error\n    .format(err_tb_str)))\nray.tune.error.TuneError: Trial raised an exception. Traceback:\nray_worker (pid=303, host=59a132d236e4)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 92, in run\n    self._entrypoint()\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 140, in entrypoint\n    return self._trainable_func(config, self._status_reporter)\n  File \"/opt/conda/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 266, in _trainable_func\n    output = train_func(config)\n  File \"<ipython-input-27-7ca1dfcabe9c>\", line 3, in train_mnist\nNameError: name 'ConvNet' is not defined\n\n\n","name":"stderr"},{"output_type":"stream","text":"== Status ==\nUsing FIFO scheduling algorithm.\nResources requested: 0/4 CPUs, 0/0 GPUs\nMemory usage on this node: 3.2/20.0 GB\nResult logdir: /root/ray_results/train_mnist\nNumber of trials: 3 ({'ERROR': 3})\nERROR trials:\n - train_mnist_0_lr=0.001:\tERROR, 1 failures: /root/ray_results/train_mnist/train_mnist_0_lr=0.001_2020-08-17_09-39-20ud0mauc3/error_2020-08-17_09-39-26.txt\n - train_mnist_1_lr=0.01:\tERROR, 1 failures: /root/ray_results/train_mnist/train_mnist_1_lr=0.01_2020-08-17_09-39-20ctumttjq/error_2020-08-17_09-39-25.txt\n - train_mnist_2_lr=0.1:\tERROR, 1 failures: /root/ray_results/train_mnist/train_mnist_2_lr=0.1_2020-08-17_09-39-203sw591ou/error_2020-08-17_09-39-25.txt\n\n","name":"stdout"},{"output_type":"error","ename":"TuneError","evalue":"('Trials did not complete', [train_mnist_0_lr=0.001, train_mnist_1_lr=0.01, train_mnist_2_lr=0.1])","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-1cc4010539ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best config: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init, sync_function)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_mnist_0_lr=0.001, train_mnist_1_lr=0.01, train_mnist_2_lr=0.1])"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a dataframe for analyzing trial results.\ndf = analysis.dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Sherpa <a id=\"8\"></a> <br>\n![](https://camo.githubusercontent.com/3e051525488a679b1489251621ab906bb66b597d/68747470733a2f2f646f63732e676f6f676c652e636f6d2f64726177696e67732f642f652f32504143582d317652615450356435577154344b59345635376e6949347746446b7a303039387a4854527a5a396e37537a7a4674644e35616b42643735486368426e6859492d4750765f415948317a5961304f325f302f7075623f773d35323226683d313530)\n\nSherpa can automatically run parallel evaluations on a cluster using a job scheduler such as SGE. Simply provide a Python script that takes a set of hyperparameters as arguments and performs a single trial evaluation. A database collects the partial results in real-time, and the hyperparameter optimization algorithm decides what to do next.\n\nSHERPA is a Python library for hyperparameter tuning of machine learning models. It provides:\n\n* Hyperparameter optimization for machine learning researchers\n* It can be used with any Python machine learning library such as Keras, Tensorflow, PyTorch, or Scikit-Learn\n* A choice of hyperparameter optimization algorithms such as Bayesian optimization via GPyOpt, Asynchronous Successive Halving (aka Hyperband) , and Population Based Training .\n* Parallel computation that can be fitted to the user's needs\n* A live dashboard for the exploratory analysis of results."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install parameter-sherpa","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting parameter-sherpa\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/7f/58250c8ca0c0c2a2f4a8d9692ac2ae75ff6f4f111c8db7c63eb668ef1f3e/parameter-sherpa-1.0.6.tar.gz (513kB)\n\u001b[K     |████████████████████████████████| 522kB 416kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pandas>=0.20.3 in /opt/conda/lib/python3.6/site-packages (from parameter-sherpa) (0.25.0)\nRequirement already satisfied: pymongo>=3.5.1 in /opt/conda/lib/python3.6/site-packages (from parameter-sherpa) (3.9.0)\nRequirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.6/site-packages (from parameter-sherpa) (1.17.0)\nRequirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from parameter-sherpa) (1.2.1)\nRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from parameter-sherpa) (0.21.3)\nRequirement already satisfied: flask>=0.12.2 in /opt/conda/lib/python3.6/site-packages (from parameter-sherpa) (1.1.1)\nCollecting GPyOpt>=1.2.5 (from parameter-sherpa)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/be/669d505416d7e465b2aef7df3b58d590f56468c4f7dc50c91fe91b8a78d9/GPyOpt-1.2.6.tar.gz (56kB)\n\u001b[K     |████████████████████████████████| 61kB 3.7MB/s eta 0:00:011\n\u001b[?25hCollecting enum34 (from parameter-sherpa)\n  Downloading https://files.pythonhosted.org/packages/63/f6/ccb1c83687756aeabbf3ca0f213508fcfb03883ff200d201b3a4c60cedcc/enum34-1.1.10-py3-none-any.whl\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from parameter-sherpa) (3.0.3)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.3->parameter-sherpa) (2.8.0)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.3->parameter-sherpa) (2019.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.19.1->parameter-sherpa) (0.13.2)\nRequirement already satisfied: Jinja2>=2.10.1 in /opt/conda/lib/python3.6/site-packages (from flask>=0.12.2->parameter-sherpa) (2.10.1)\nRequirement already satisfied: click>=5.1 in /opt/conda/lib/python3.6/site-packages (from flask>=0.12.2->parameter-sherpa) (7.0)\nRequirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.6/site-packages (from flask>=0.12.2->parameter-sherpa) (1.1.0)\nRequirement already satisfied: Werkzeug>=0.15 in /opt/conda/lib/python3.6/site-packages (from flask>=0.12.2->parameter-sherpa) (0.15.5)\nCollecting GPy>=1.8 (from GPyOpt>=1.2.5->parameter-sherpa)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/976598f98adbfa918a480cb2d643f93fb555ca5b6c5614f76b69678114c1/GPy-1.9.9.tar.gz (995kB)\n\u001b[K     |████████████████████████████████| 1.0MB 3.0MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->parameter-sherpa) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->parameter-sherpa) (1.1.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->parameter-sherpa) (2.4.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.20.3->parameter-sherpa) (1.12.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=2.10.1->flask>=0.12.2->parameter-sherpa) (1.1.1)\nCollecting paramz>=0.9.0 (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/37/4abbeb78d30f20d3402887f46e6e9f3ef32034a9dea65d243654c82c8553/paramz-0.9.5.tar.gz (71kB)\n\u001b[K     |████████████████████████████████| 71kB 4.4MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->parameter-sherpa) (41.0.1)\nRequirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.6/site-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (4.4.0)\nBuilding wheels for collected packages: parameter-sherpa, GPyOpt, GPy, paramz\n  Building wheel for parameter-sherpa (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for parameter-sherpa: filename=parameter_sherpa-1.0.6-py2.py3-none-any.whl size=542120 sha256=9ff7882f137ecf22fdb9996d175b26b6df99d8f6cae0eaff63b27cfa5df6646f\n  Stored in directory: /root/.cache/pip/wheels/a3/a5/0e/50f662d278b580f9ed20f6ea3c129e8d09eda95cf45666063c\n  Building wheel for GPyOpt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPyOpt: filename=GPyOpt-1.2.6-cp36-none-any.whl size=83622 sha256=d1f830efbcae42eb62e3b67aac0b815600801600a7f7239702851dac1d586378\n  Stored in directory: /root/.cache/pip/wheels/b2/00/69/cfa967a125cf25e66f644be6193ad6f0edf231147879ad714f\n  Building wheel for GPy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPy: filename=GPy-1.9.9-cp36-cp36m-linux_x86_64.whl size=2704613 sha256=ef06077a223d56e443495d76deb95f6cd0e6385c1aac09a49390e8d61aba3d61\n  Stored in directory: /root/.cache/pip/wheels/5d/36/66/2b58860c84c9f2b51615da66bfd6feeddbc4e04d887ff96dfa\n  Building wheel for paramz (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for paramz: filename=paramz-0.9.5-cp36-none-any.whl size=102552 sha256=b1d83c17849062227079ad7f6f0c8cbfed54d243fe49f1de249efde3d25961f6\n  Stored in directory: /root/.cache/pip/wheels/c8/4a/0e/6e0dc85541825f991c431619e25b870d4b812c911214690cf8\nSuccessfully built parameter-sherpa GPyOpt GPy paramz\nInstalling collected packages: paramz, GPy, GPyOpt, enum34, parameter-sherpa\nSuccessfully installed GPy-1.9.9 GPyOpt-1.2.6 enum34-1.1.10 parameter-sherpa-1.0.6 paramz-0.9.5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport time\nimport sherpa\nimport sherpa.algorithms.bayesian_optimization as bayesian_optimization","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = [sherpa.Discrete('n_estimators', [2, 50]),\n              sherpa.Choice('criterion', ['gini', 'entropy']),\n              sherpa.Continuous('max_features', [0.1, 0.9])]\n\nalgorithm = bayesian_optimization.GPyOpt(max_concurrent=1,model_type='GP_MCMC',acquisition_type='EI_MCMC',max_num_trials=10)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = load_breast_cancer(return_X_y=True)\nstudy = sherpa.Study(parameters=parameters,\n                     algorithm=algorithm,\n                     lower_is_better=False)\n\nfor trial in study:\n    print(\"Trial \", trial.id, \" with parameters \", trial.parameters)\n    clf = RandomForestClassifier(criterion=trial.parameters['criterion'],\n                                 max_features=trial.parameters['max_features'],\n                                 n_estimators=trial.parameters['n_estimators'],\n                                 random_state=0)\n    scores = cross_val_score(clf, X, y, cv=5)\n    print(\"Score: \", scores.mean())\n    study.add_observation(trial, iteration=1, objective=scores.mean())\n    study.finalize(trial)\nprint(study.get_best_result())","execution_count":4,"outputs":[{"output_type":"stream","text":" * Serving Flask app \"sherpa.app.app\" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: on\nTrial  1  with parameters  {'n_estimators': 44, 'criterion': 'gini', 'max_features': 0.6633957976993129}\nScore:  0.9615236629472875\nTrial  2  with parameters  {'n_estimators': 48, 'criterion': 'entropy', 'max_features': 0.34083114756585875}\nScore:  0.9614928818776451\nTrial  3  with parameters  {'n_estimators': 28, 'criterion': 'gini', 'max_features': 0.7098315407766511}\nScore:  0.9615236629472875\nTrial  4  with parameters  {'n_estimators': 6, 'criterion': 'entropy', 'max_features': 0.8785580904068298}\nScore:  0.9597537514428627\nTrial  5  with parameters  {'n_estimators': 46, 'criterion': 'gini', 'max_features': 0.1}\nScore:  0.9597229703732204\nTrial  6  with parameters  {'n_estimators': 28, 'criterion': 'gini', 'max_features': 0.7102610252946}\nScore:  0.9615236629472875\nTrial  7  with parameters  {'n_estimators': 28, 'criterion': 'gini', 'max_features': 0.469858723118209}\nScore:  0.9614928818776451\nTrial  8  with parameters  {'n_estimators': 43, 'criterion': 'gini', 'max_features': 0.9}\nScore:  0.96326279338207\nTrial  9  with parameters  {'n_estimators': 42, 'criterion': 'gini', 'max_features': 0.9}\nScore:  0.9632935744517124\nTrial  10  with parameters  {'n_estimators': 42, 'criterion': 'gini', 'max_features': 0.9}\nScore:  0.9632935744517124\n{'Trial-ID': 9, 'Iteration': 1, 'criterion': 'gini', 'max_features': 0.9, 'n_estimators': 42, 'Objective': 0.9632935744517124}\n","name":"stdout"}]},{"metadata":{"_cell_guid":"d4a7247b-6654-483b-b8d5-a754c7834efe","_uuid":"78e8638406e22164d84aec94c2742c92da1f19ce"},"cell_type":"markdown","source":"So the best hyperparameters are **'Trial-ID': 5, 'Iteration': 1, 'criterion': 'entropy', 'max_features': 0.3942116305734421, 'n_estimators': 38, 'Objective': 0.9632012312427858**\n\n# 9. Conclusion <a id=\"9\"></a> <br>\n\n**So by now I hope you had a fair understanding of how to do Hyperparameter Tuning with open source libraries as mentioned above.**\n\n# Greatly Appreciate to leave your comments/feedback and If you like this kernel please kindly do <font color=\"red\">UPVOTE."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}