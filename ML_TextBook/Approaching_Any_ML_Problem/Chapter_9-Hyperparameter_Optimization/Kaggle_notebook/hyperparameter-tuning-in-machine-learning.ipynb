{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018165,
     "end_time": "2020-09-30T11:50:49.353828",
     "exception": false,
     "start_time": "2020-09-30T11:50:49.335663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hyperparameter tuning is a very essential part of our machine learning project, with good hyperparameters we can achieve a very good result for our machine learning model. To optimize our machine leanring model performance, we must tune the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015982,
     "end_time": "2020-09-30T11:50:49.386560",
     "exception": false,
     "start_time": "2020-09-30T11:50:49.370578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**What is hyperparameter ?**\n",
    "\n",
    "A hyperparameter is a variable in our machine learning algorithm which we can not learn by training the model and its value can not be estimated based on data. We have to simply put the values according to over model performance and see for which value of hyperparameters the model is giving the best result.\n",
    "\n",
    "Hyperparameter tuning is a very time-consuming process in the machine learning pipeline if we do it manually and try out every possible combination of hyperparameters for the machine learning algorithm. There are various methods and libraries are available for this task such as :\n",
    "\n",
    "- GridSearchCV \n",
    "\n",
    "- RandomizedSearchCV\n",
    "\n",
    "- Bayesian Optimization\n",
    "\n",
    "- Sequential Model Based Optimization(Tuning a scikit-learn estimator with skopt)\n",
    "\n",
    "- Optuna\n",
    "\n",
    "- Genetic Algorithms \n",
    "\n",
    "- Hyperopt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015715,
     "end_time": "2020-09-30T11:50:49.419016",
     "exception": false,
     "start_time": "2020-09-30T11:50:49.403301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook, I am going to discuss how we can tune hyperparameters using GridSearchCV, RandomizedSearchCV, and Bayesian Optimization for various machine learning algorithms and discuss how these methods work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-09-30T11:50:49.459218Z",
     "iopub.status.busy": "2020-09-30T11:50:49.458343Z",
     "iopub.status.idle": "2020-09-30T11:50:49.462395Z",
     "shell.execute_reply": "2020-09-30T11:50:49.461672Z"
    },
    "papermill": {
     "duration": 0.027402,
     "end_time": "2020-09-30T11:50:49.462574",
     "exception": false,
     "start_time": "2020-09-30T11:50:49.435172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T11:50:49.503023Z",
     "iopub.status.busy": "2020-09-30T11:50:49.502067Z",
     "iopub.status.idle": "2020-09-30T11:50:51.108906Z",
     "shell.execute_reply": "2020-09-30T11:50:51.108255Z"
    },
    "papermill": {
     "duration": 1.628124,
     "end_time": "2020-09-30T11:50:51.109043",
     "exception": false,
     "start_time": "2020-09-30T11:50:49.480919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets #import datasets to use in this notebook\n",
    "from sklearn import model_selection #model_selection provides the methods for hyperparameter tuning\n",
    "from skopt import BayesSearchCV #importing BayesSearchCV for Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016655,
     "end_time": "2020-09-30T11:50:51.142467",
     "exception": false,
     "start_time": "2020-09-30T11:50:51.125812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    Following evaluation metrics are available in sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T11:50:51.184239Z",
     "iopub.status.busy": "2020-09-30T11:50:51.183285Z",
     "iopub.status.idle": "2020-09-30T11:50:51.188917Z",
     "shell.execute_reply": "2020-09-30T11:50:51.188024Z"
    },
    "papermill": {
     "duration": 0.029871,
     "end_time": "2020-09-30T11:50:51.189108",
     "exception": false,
     "start_time": "2020-09-30T11:50:51.159237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01811,
     "end_time": "2020-09-30T11:50:51.229040",
     "exception": false,
     "start_time": "2020-09-30T11:50:51.210930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I am using boston house price data for in this notebook for Regression algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T11:50:51.273282Z",
     "iopub.status.busy": "2020-09-30T11:50:51.272304Z",
     "iopub.status.idle": "2020-09-30T11:50:51.315068Z",
     "shell.execute_reply": "2020-09-30T11:50:51.313988Z"
    },
    "papermill": {
     "duration": 0.068815,
     "end_time": "2020-09-30T11:50:51.315249",
     "exception": false,
     "start_time": "2020-09-30T11:50:51.246434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset : (506, 13)\n",
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  \n",
      "0     15.3  396.90   4.98  \n",
      "1     17.8  396.90   9.14  \n",
      "2     17.8  392.83   4.03  \n",
      "3     18.7  394.63   2.94  \n",
      "4     18.7  396.90   5.33  \n"
     ]
    }
   ],
   "source": [
    "#loading bostonhousing dataset for regression problems\n",
    "boston_data = datasets.load_boston()\n",
    "r_X = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "r_y = boston_data.target\n",
    "print(f\"Shape of the dataset : {r_X.shape}\")\n",
    "print(r_X.head())\n",
    "\n",
    "#applying standard scaling on the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_r = StandardScaler()\n",
    "sc_r_X = sc_r.fit_transform(r_X)\n",
    "\n",
    "#split the dataset into test and train \n",
    "from sklearn.model_selection import train_test_split\n",
    "#taking 20% data as test data and 80% data as train data\n",
    "r_X_train,r_X_test,r_Y_train,r_Y_test = train_test_split(sc_r_X,r_y, test_size=0.20, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017417,
     "end_time": "2020-09-30T11:50:51.350883",
     "exception": false,
     "start_time": "2020-09-30T11:50:51.333466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Using Iris flower dataset for classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T11:50:51.395695Z",
     "iopub.status.busy": "2020-09-30T11:50:51.394880Z",
     "iopub.status.idle": "2020-09-30T11:50:51.414633Z",
     "shell.execute_reply": "2020-09-30T11:50:51.413568Z"
    },
    "papermill": {
     "duration": 0.046068,
     "end_time": "2020-09-30T11:50:51.414801",
     "exception": false,
     "start_time": "2020-09-30T11:50:51.368733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset : (150, 4)\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n"
     ]
    }
   ],
   "source": [
    "#loading Iris dataset for regression problems\n",
    "iris_data = datasets.load_iris()\n",
    "c_X = pd.DataFrame(iris_data.data,columns=iris_data.feature_names)\n",
    "c_y = iris_data.target\n",
    "print(f\"Shape of the dataset : {c_X.shape}\")\n",
    "print(c_X.head())\n",
    "\n",
    "#applying standard scaling on the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_c = StandardScaler()\n",
    "sc_c_X = sc_c.fit_transform(c_X)\n",
    "\n",
    "#split the dataset into test and train \n",
    "from sklearn.model_selection import train_test_split\n",
    "#taking 20% data as test data and 80% data as train data\n",
    "c_X_train,c_X_test,c_Y_train,c_Y_test = train_test_split(sc_c_X,c_y, test_size=0.20, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019168,
     "end_time": "2020-09-30T11:50:51.454532",
     "exception": false,
     "start_time": "2020-09-30T11:50:51.435364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### GridSearchCV :\n",
    "GridSearchCV uses every possible combination of given entries for our hyperparameters and gives the output for which set of hyperparameter our model performs best. This method takes very much time because it tries out each and every possible combination. Here we discuss how we can use GridSearchCV using sklearn library.<br>\n",
    "\n",
    "\n",
    "Some important parameters and attributes which are important for our model :<br>\n",
    "\n",
    " ***Parameters*** :\n",
    " - estimatior : In this parameter, we pass the function of our model for which we want to tune hyperparameters. The information is only for sklearn library.\n",
    " - param_grid : This parameter takes a dictionary which consist of hyperparameter name and their values.\n",
    " - scoring : This parameter take a string which define for what evaluation matrics we want to optimize our model.\n",
    " - cv : Number of cross validations we want split our training dataset. Default value is 5. <br>\n",
    " \n",
    " ***Attributes***\n",
    " - best_estimator_ : It provide which estimatior gives best result for letf-out data.\n",
    " - best_params_ : Parameter setting that gave the best results on the hold out data.\n",
    " - best_score_ : Mean cross-validated score of the best_estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018088,
     "end_time": "2020-09-30T11:50:51.491087",
     "exception": false,
     "start_time": "2020-09-30T11:50:51.472999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we discuss how we can tune **Ridge_regression hyperparameters** uning GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-09-30T11:50:51.537771Z",
     "iopub.status.busy": "2020-09-30T11:50:51.536598Z",
     "iopub.status.idle": "2020-09-30T11:50:53.582981Z",
     "shell.execute_reply": "2020-09-30T11:50:53.582091Z"
    },
    "papermill": {
     "duration": 2.073533,
     "end_time": "2020-09-30T11:50:53.583126",
     "exception": false,
     "start_time": "2020-09-30T11:50:51.509593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=3, estimator=Ridge(), n_jobs=-1,\n",
      "             param_grid={'alpha': [0.01, 0.1, 1.0, 10, 100],\n",
      "                         'fit_intercept': [True, False],\n",
      "                         'normalize': [True, False]},\n",
      "             scoring='r2')\n",
      "Best Estimator is : Ridge(alpha=0.01, normalize=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "R_reg = Ridge()\n",
    "#parameter grid dictionary\n",
    "params = {\n",
    "    'alpha' : [0.01, 0.1, 1.0, 10, 100],\n",
    "    'fit_intercept' : [True,False],\n",
    "    'normalize' : [True,False]\n",
    "}\n",
    "#r2_score is choosen as evaluation matrics\n",
    "gs = model_selection.GridSearchCV(estimator=R_reg,param_grid=params,scoring='r2',cv=3,n_jobs = -1)\n",
    "print(gs)\n",
    "#fit our traing data to GridSearchCV\n",
    "gs.fit(r_X_train,r_Y_train)\n",
    "best_est = gs.best_estimator_\n",
    "print(f\"Best Estimator is : {best_est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02021,
     "end_time": "2020-09-30T11:50:53.625318",
     "exception": false,
     "start_time": "2020-09-30T11:50:53.605108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we can see for alpha = 0.01 and normalise = True our model gives best r2_score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019963,
     "end_time": "2020-09-30T11:50:53.666835",
     "exception": false,
     "start_time": "2020-09-30T11:50:53.646872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RandomizedSearchCV :\n",
    "RandomizedSearchCV generates all possible combination of given hyperparameters and then randomly choose fix numbers of combinations. How many combinations of hyperparameters to be used is given by us. It trains the model for those hyperparameter combinations and use cross-validation technique to measure the accuracy and return the best estimator. It takes less time then GridSearchCV.<br>\n",
    "\n",
    "\n",
    "Some important parameters and attributes which are important for our model :<br>\n",
    "\n",
    " ***Parameters*** :\n",
    " - n_iter : It is the parameter that decides for how much combinations model is going to be trained. The default value is 10. If the value fo n_iter is high there may be a high chance we get the best estimator.\n",
    " - estimatior : In this parameter, we pass the function of our model for which we want to tune hyperparameters. The information is only for sklearn library.\n",
    " - param_distributions : This parameter takes a dictionary which consist of hyperparameter name and their values.\n",
    " - scoring : This parameter take a string which define for what evaluation matrics we want to optimize our model.\n",
    " - cv : Number of cross validations we want split our training dataset. Default value is 5. <br>\n",
    " \n",
    " ***Attributes***\n",
    " - best_estimator_ : It provide which estimatior gives best result for letf-out data.\n",
    " - best_params_ : Parameter setting that gave the best results on the hold out data.\n",
    " - best_score_ : Mean cross-validated score of the best_estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020865,
     "end_time": "2020-09-30T11:50:53.708030",
     "exception": false,
     "start_time": "2020-09-30T11:50:53.687165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here I am going to discuss we can tune hyperparameters for **support vector machine** classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T11:50:53.768201Z",
     "iopub.status.busy": "2020-09-30T11:50:53.767000Z",
     "iopub.status.idle": "2020-09-30T11:50:53.927886Z",
     "shell.execute_reply": "2020-09-30T11:50:53.927216Z"
    },
    "papermill": {
     "duration": 0.198848,
     "end_time": "2020-09-30T11:50:53.928029",
     "exception": false,
     "start_time": "2020-09-30T11:50:53.729181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV(cv=3, estimator=SVC(probability=True), n_iter=20, n_jobs=-1,\n",
      "                   param_distributions={'C': [0.1, 1, 10, 100, 1000],\n",
      "                                        'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
      "                                        'kernel': ['linear', 'rbf']},\n",
      "                   scoring='neg_log_loss')\n",
      "Best Estimator is : SVC(C=100, gamma=1, kernel='linear', probability=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.SVC(probability=True)\n",
    "#parameter grid dictionary\n",
    "params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],  \n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "    'kernel': ['linear','rbf'] #other kernels are ‘sigmoid’, ‘precomputed’, 'ploy'\n",
    "    \n",
    "}\n",
    "#neg_log_loss is choosen as evaluation matrics\n",
    "#here I am using n_iter = 20 only you can choose more\n",
    "gs = model_selection.RandomizedSearchCV(n_iter=20,estimator=svm_clf,param_distributions=params,scoring='neg_log_loss',cv=3,n_jobs = -1)\n",
    "print(gs)\n",
    "#fit our traing data to RandomizedSearchCV\n",
    "gs.fit(c_X_train,c_Y_train)\n",
    "best_est = gs.best_estimator_\n",
    "print(f\"Best Estimator is : {best_est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028642,
     "end_time": "2020-09-30T11:50:53.986852",
     "exception": false,
     "start_time": "2020-09-30T11:50:53.958210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bayesian Optimization :\n",
    "Bayesian optimization works fast and provides more efficient results compare to GridSearchCV and RandomizedSearchCV. This technique is based on Bayes theorem and provides a probabilistically principled method for global optimization. Bayesian Optimization keeps tracks of past combinations that we previously used and according to that, it provides hyperparameter to the model in such a manner that results in improves in each iteration.<br>\n",
    "\n",
    "***Baysian optimzation is not present in sklearn library for this we use skopt library.***\n",
    "\n",
    "Some important parameters and attributes which are important for our model :<br>\n",
    "\n",
    " - n_iter : It is the parameter that decides for how much combinations model is going to be trained. The default value is 50. If the value fo n_iter is high there may be a high chance we get the best estimator.\n",
    " - estimatior : In this parameter, we pass the function of our model for which we want to tune hyperparameters. The information is only for sklearn library.\n",
    " - search_spaces : This parameter takes a dictionary which consist of hyperparameter name and their values.\n",
    " - optimizer_kwargs : Dictionary of arguments passed to Optimizer. For example, {'base_estimator': 'RF'} would use a Random Forest surrogate instead of the default Gaussian Process.\n",
    " - cv : Number of cross validations we want to split our training dataset. Default value is 3. <br>\n",
    " \n",
    " ***Attributes***\n",
    " - best_estimator_ : It provide which estimatior gives best result for letf-out data.\n",
    " - best_params_ : Parameter setting that gave the best results on the hold out data.\n",
    " - best_score_ : Mean cross-validated score of the best_estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03515,
     "end_time": "2020-09-30T11:50:54.050642",
     "exception": false,
     "start_time": "2020-09-30T11:50:54.015492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Hyperparameter Tuning of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-30T11:50:54.129956Z",
     "iopub.status.busy": "2020-09-30T11:50:54.127963Z",
     "iopub.status.idle": "2020-09-30T11:55:16.205999Z",
     "shell.execute_reply": "2020-09-30T11:55:16.206936Z"
    },
    "papermill": {
     "duration": 262.12321,
     "end_time": "2020-09-30T11:55:16.207143",
     "exception": false,
     "start_time": "2020-09-30T11:50:54.083933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
      "              optimizer_kwargs={'base_estimator': 'RF'},\n",
      "              search_spaces={'bootstrap': [True, False],\n",
      "                             'max_depth': [5, 10, 15, 20, 25, 30, 35, 40, 45,\n",
      "                                           50],\n",
      "                             'max_features': [4, 'sqrt', 'log2'],\n",
      "                             'min_samples_leaf': [1, 3, 5, 7, 9, 10],\n",
      "                             'min_samples_split': [3, 4, 6, 7, 8, 9, 10],\n",
      "                             'n_estimators': [50, 100, 150, 200, 250, 300, 350,\n",
      "                                              400, 450, 500, 550, 600, 650, 700,\n",
      "                                              750, 800, 850, 900, 950, 1000]})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/opt/conda/lib/python3.7/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/opt/conda/lib/python3.7/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator is : RandomForestClassifier(max_depth=50, max_features='log2', min_samples_leaf=7,\n",
      "                       min_samples_split=8, n_estimators=500)\n",
      "Best Score is : 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "#importing skopt library for optimization\n",
    "import skopt\n",
    "from sklearn import ensemble\n",
    "\n",
    "rf_clf = ensemble.RandomForestClassifier()\n",
    "#parameter grid dictionary\n",
    "params = {\n",
    "   'n_estimators': [int(x) for x in np.linspace(50,1000, num=20)], #this return a list with 20 equally spaced numbers between 50 and 1000\n",
    "   'max_features': [len(c_X.columns), \"sqrt\", \"log2\"],\n",
    "   'max_depth': [int(x) for x in np.linspace(5,50,num=10)],\n",
    "   'min_samples_split': [3,4,6,7,8,9,10],\n",
    "   'min_samples_leaf': [1,3,5,7,9,10],\n",
    "   'bootstrap': [True,False]\n",
    "    \n",
    "}\n",
    "\n",
    "#here I am using n_iter = 50 only you can choose more\n",
    "gs = skopt.BayesSearchCV(n_iter=50,estimator=rf_clf,search_spaces=params,optimizer_kwargs={'base_estimator': 'RF'},cv=5,n_jobs = -1)\n",
    "print(gs)\n",
    "#fit our traing data to BayesSearchCV\n",
    "gs.fit(c_X_train,c_Y_train)\n",
    "best_est = gs.best_estimator_\n",
    "best_score = gs.best_score_\n",
    "print(f\"Best Estimator is : {best_est}\")\n",
    "print(f\"Best Score is : {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022391,
     "end_time": "2020-09-30T11:55:16.253104",
     "exception": false,
     "start_time": "2020-09-30T11:55:16.230713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*So in this notebook, I discussed some common approaches for hyperparameter tuning which you can be applied in your machine learning algorithm optimization. If you find this notebook useful upvote the note or some mistake in the notebook please mention it in the comment section.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022613,
     "end_time": "2020-09-30T11:55:16.298737",
     "exception": false,
     "start_time": "2020-09-30T11:55:16.276124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**References** :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02257,
     "end_time": "2020-09-30T11:55:16.344669",
     "exception": false,
     "start_time": "2020-09-30T11:55:16.322099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 272.466581,
   "end_time": "2020-09-30T11:55:16.475905",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-30T11:50:44.009324",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
